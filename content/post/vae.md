---
title: "Beyond the Gaussian Variational Encoder"
date: 2021-10-20T22:23:02+02:00
Description: ""
math : true
Tags: []
Categories: []
DisableComments: true
draft: true
---

When using the reparametrization trick in variational encoders (VAE), it is common to assume that both the encoder and the decoder follow a Gaussian distribution. This is mostly due to practical reasons:  most libraries provide an implementation of the Gaussian distribution and it is relatively easy to get the analytical form of the Kullback-Leibler divergence. 

However, the Gaussian variational approximation is just a special case of the original Kingma & Welling paper from 2014. Kingma & Welling (2014) outline three basic approaches for the reparametrization trick:

1.  **Location-scale distributions:** this includes distributions of the form \\(Y = \mu + \sigma \cdot \varepsilon \\), where \\(\mu\\) is the location parameter, \\(\sigma\\) is the scale parameter and \\(\varepsilon\\) is a sample of the standard distribution. In this case, the standard distribution has \\(\mu = 0\\) and \\(\sigma = 1\\) and is therefore parameter-free. This includes distributions such as the Laplace, Student's-\\(t\\), Logistic, Uniform and Gaussian.  
2. **Distributions with tractable quantile function:** if the quantile function (also called inverse CDF) is tractable, we can sample \\(u\\) uniformly from the unit interval (i.e., \\(u \sim \mathcal U(0, 1)\\), which is not necessarily restricted to the unidimensional case). Then \\(X = CDF^{-1}(u)\\) is the sample from the desired distribution. Some distributions in this category are Exponential, Cauchy, Logistic, Weibull, Gumbel. 
3. **Composition**: some distribution are generated by transforming other distributions. A classical example is the Log-Normal, which is generated by exponentiating a Gaussian random variable (category 1). Another example is the Gamma distribution, which can be obtained by adding Exponential random variables (category 2).  

By using a standard distribution (category 1) or sampling from the unit interval (category 2), the random part of our model is parameter free and there is nothing in our neural network blocking the backpropagation step. This reasoning also applies to category 3, because it is based on categories 1 and 2. 

However, the reparametrization trick does not solve all of our problems. We still need the KL-divergence between the distributions that we are using. As calculating the analytical KL-divergence is often intractable, it is important to restrict our choices to distributions for which we already have the KL-divergence. Wikipedia provides the analytical KL-divergence for some distributions and [this dissertation](https://www.mast.queensu.ca/~communications/Papers/gil-msc11.pdf)  contains the KL-divergence for many common distributions. 

## A simple Gaussian VAE

Following Kingma & Welling (2014), the aim is to maximize the ELBO

\\(\mathcal L  (\theta, \phi; x^{(i)}) = -D_{KL}(q_{\phi}(z|x^{(i)})||p_{\theta}(z)) + \log p_\theta (x^{(i)}|z^{(i)}) \\),

where \\(q_{\phi}(z|x^{(i)})\\) is the encoder parametrized by the vector \\(\phi\\), \\(p_\theta(z)\\) is the prior over the embeddings \\(z\\), \\(p_\theta (x^{(i)}|z^{(i)})\\) is the likelihood of the decoder for observation \\(x^{(i)}\\) given the embedding and \\(D_{KL}\\) is the Kullback-Leibler divergence. \\(z^{(i)}\\) is defined as \\(g_\phi(\varepsilon^{(i)}, x^{(i)})\\), where \\(\varepsilon\\) follows some distribution \\(p(\varepsilon)\\). The term \\( \log p_\theta (x^{(i)}|z^{(i)}) \\) is also called _reconstruction error_. Note that the original paper also includes a dimension \\(L\\), which indicates the number of samples generated per observation. This is ignored for simplicity. 

For a Gaussian VAE, we follow the first alternative of the reparametrization trick. We define \\(z = \mu + \sigma \cdot \varepsilon\\), with \\(\varepsilon \sim \mathcal N (0, I_k)\\). \\(I_k\\) is the identity matrix where \\(k\\) is the size of the embedding \\(z\\). The parameters \\(\mu\\) and \\(\sigma\\) are learned from fully connected \\(k\\)-dimensional layers. The model is defined as follows:

\\( H_1 = h(W_1X), ~~ H_2 = h(W_2H_1),\\)

\\( \mu = h(W_3H_2), ~~ \log \sigma = h(W_4H_2),\\)

\\( z = \mu + \sigma \cdot \varepsilon, ~~ \varepsilon \sim \mathcal N (0, I_k) \\)

\\( H_3 = h(W_5z), ~~ Y = h(W_6H_3)\\)

Here, \\(h\\) denotes an activation function such as ReLU. Moreover, the network can be made deeper or flatter, if necessary. Assuming the parameter-free prior \\(p_\theta(z) = \mathcal N (0, 1)\\), then the Kullback-Leibler \\(D_{KL}(\mathcal N(\mu, \sigma) ||\mathcal N(0, 1)) \\) has a very simple form which can be found [in this Wikipedia article](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence#Examples). The reconstruction error (which is also the likelihood of the decoder) is not used if we just want to get the embeddings \\(z\\), but is still needed for training. A simple specification would be \\(x^{(i)}|z^{(i)} \sim \mathcal N (y^{(i)}, \tilde \sigma)\\), where \\(y^{(i)}\\) is the prediction from the decoder for observation \\(i\\) and \\(\tilde \sigma \\) is and additional learnable parameter. However, we could choose a different and more complex distribution for the decoder. 

An implementation with Pytorch Lightning can be written as follows:

```python
import torch
from torch import nn
import pytorch_lightning as pl

class GaussianVAE(pl.LightningModule):
    def __init__(self, size, latent=2):
        super().__init__()
        self.encoder = nn.Sequential(
            nn.Linear(size, 64), nn.ReLU(), nn.Linear(64, latent)
        )
        self.decoder = nn.Sequential(
            nn.Linear(latent, 64), nn.ReLU(), nn.Linear(64, size)
        )
        self.mu = nn.Linear(latent, latent)
        self.log_sigma = nn.Linear(latent, latent)

        # For reconstruction loss
        self.log_scale = nn.Parameter(torch.Tensor([0.0]))

    def forward(self, x):
        h = self.encoder(x)
        return self.reparametrize(self.mu(h), self.log_sigma(h))

    def reparametrize(self, mu, log_scale):
        scale = torch.exp(log_scale)
        epsilon = torch.randn_like(scale)
        return mu + scale * epsilon

    def kl(self, mu, log_sigma):
        var = log_sigma.exp() ** 2
        return torch.mean(
            -0.5 * torch.sum(1 + var.log() - mu.pow(2) - var, dim=1),
            dim=0,
        )

    def training_step(self, batch, batch_idx):
        x, _ = batch
        x = x.view(x.size(0), -1).float() # flatten input
        
        # hidden representation
        h = self.encoder(x) 
        
        # parameters for encoder distribution 
        mu = self.mu(h)
        log_scale = self.log_sigma(h)
        
        # variational embedding
        z = self.reparametrize(mu, log_scale) 
        
        # decoded embedding
        y_hat = self.decoder(z)

        # decoder distribution with global scale parameter
        decoder_distr = torch.distributions.Normal(y_hat, self.log_scale.exp())
        
        # calculate ELBO
        reconstruction_loss = decoder_distr.log_prob(x).sum()
        kl = self.kl(mu, log_scale)
        
        # flipped signed to minimize negative ELBO
        neg_elbo = kl - reconstruction_loss
        # Log to TensorBoard
        self.log("elbo", neg_elbo)
        self.log("kl", kl)
        self.log("reconstruction", reconstruction_loss)
        self.log("Global sd", self.log_scale.exp())
        return neg_elbo

    def configure_optimizers(self):
        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)
        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer)
        return {
            "optimizer": optimizer,
            "lr_scheduler": scheduler,
            "monitor": "elbo",
        }
```

We train the decoder with the MNIST data set, which has images of size 28 \\(\cdot\\) 28. 

```python
import sys
sys.path.append(".")
import os
import torch
from torchvision import transforms
from torchvision.datasets import MNIST
from torch.utils.data import DataLoader, random_split
import pytorch_lightning as pl

dataset = MNIST(os.getcwd(), download=True, transform=transforms.ToTensor())
train_loader = DataLoader(random_split(dataset, [2000, 58000])[0])

vae = VAE(28 * 28)
trainer = pl.Trainer(max_epochs=3)
```

## Defining a Gumbel VAE

Now that we have defined a simple Gaussian VAE, we will use the [Gumbel distribution](https://en.wikipedia.org/wiki/Gumbel_distribution) as a variational approximation. The Gumbel distribution is part of the second category of distributions, for which we use the quantile function in the reparametrization trick. This distribution is usually used to model the maximum of a number of samples. Like the normal distribution it has a location parameter \\(\mu\\) and a positive scale parameter \\(\beta\\). However, it has a constant positive skewness approximately equal to 1.14. The support of the distribution is \\(\R\\). Fortunately, the Gumbel distribution is already implemented in Pytorch. The only thing that is missing is the KL divergence between two Gumbel distributions, which can be taken from the dissertation that was linked above:

\\(D_{KL}(Gumbel(\mu_q, \beta_q)||Gumbel(\mu_p, \beta_p)) = \log \frac{\beta_p}{\beta_q} + \gamma (\frac{\beta_q}{\beta_p} - 1) + \exp(\frac{\mu_p - \mu_q}{\beta_p})\Gamma (\frac{\beta_q}{\beta_p} + 1) - 1 \\),

where \\(\Gamma\\) is the gamma function and \\(\gamma\\) is the Euler-Mascheroni constant (\\(\approx 0.5772\\)). 

Following the reparametrization trick we first sample \\(u\\) from \\(U(0, I)\\) and then use the quantile function of \\(Gumbel(\mu_q, \beta_q)\\) to generate a sample from the variational encoder. \\(\mu_q\\) and \\(\beta_q\\) are estimated through two parallel fully connected layers of size equal to the latent dimension. We assume that the embedding \\(z\\) has a prior \\(Gumbel(\mu_p, \beta_p)\\) with \\(\mu_p = 0 \\) and \\(\beta_p = 1\\). Similar to the Gaussian VAE, we assume \\(x^{(i)}|z^{(i)} \sim Gumbel (y^{(i)}, \tilde \beta)\\), where \\(y^{(i)}\\) is the prediction from the decoder and \\(\tilde \beta \\) is and global learnable parameter. 





## Visualizing the reconstruction
